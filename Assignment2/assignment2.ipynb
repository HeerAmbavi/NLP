{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize,word_tokenize\n",
    "from collections import Counter\n",
    "import random\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Parse the dataset into sentences using sentence tokenizer and divide it into 80/20 ratio. Keep 80% dataset for training N-grams and keep 20% for test. You can filter out unnecessary symbols, newlines, etc. You can add symbols <s> and  </s> to mark sentence start and end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=open(\"aiw.txt\",\"r\")\n",
    "text=f.read().replace(\"\\n\",\" \")\n",
    "text=text.lower()\n",
    "\n",
    "#parse dataset\n",
    "sent_tokenize_list = sent_tokenize(text)\n",
    "\n",
    "#divide dataset into 80/20 ratio.\n",
    "train_dataset,test_dataset=train_test_split(sent_tokenize_list,test_size=0.2,random_state=40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "#parsing the dataset into words\n",
    "def tokenize(sent_list):\n",
    "    tokenize_list=[]\n",
    "    remove=[',','.',\"''\",'``','--','!','?',';',\"'s\",\"(\",\")\",\"’\",'‘','*','[',']',\":\",\"“\"]\n",
    "    for i in sent_list:\n",
    "        tokenize_list.append('<s>')\n",
    "        words=word_tokenize(i)\n",
    "        for j in words:\n",
    "            if j not in remove:\n",
    "                tokenize_list.append(j)\n",
    "        tokenize_list.append('</s>')    \n",
    "    return(tokenize_list)\n",
    "\n",
    "tokenize_list=tokenize(train_dataset)\n",
    "\n",
    "# count of unigrams in corpus\n",
    "unigrams=Counter(tokenize_list)\n",
    "tokens=0\n",
    "for i in unigrams.keys():\n",
    "    tokens+=unigrams[i]\n",
    "\n",
    "#thus, sze of the vocabulary for given corpus is,\n",
    "V=len(unigrams)\n",
    "\n",
    "#bigrams\n",
    "bigrams=Counter()\n",
    "for i in range(len(tokenize_list)-1):\n",
    "    b=(tokenize_list[i],tokenize_list[i+1])\n",
    "    bigrams[b]=bigrams.get(b,0)+1\n",
    "#print(bigrams)    \n",
    "        \n",
    "#trigrams\n",
    "trigrams=Counter()\n",
    "for i in range(len(tokenize_list)-2):\n",
    "    b=(tokenize_list[i],tokenize_list[i+1],tokenize_list[i+2])\n",
    "    trigrams[b]=trigrams.get(b,0)+1\n",
    "#print(trigrams)    \n",
    "\n",
    "#quadgrams\n",
    "quadgrams=Counter()\n",
    "for i in range(len(tokenize_list)-3):\n",
    "    b=(tokenize_list[i],tokenize_list[i+1],tokenize_list[i+2],tokenize_list[i+3])\n",
    "    quadgrams[b]=quadgrams.get(b,0)+1\n",
    "#print(quadgrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q2) Compute MLE for unigram, bigram, trigrams and quadgrams. How many n-grams are possible and how many actually exists? You can either use entire corpus or training corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example: unigram mle for alice: 0.014273281114012184\n",
      "Example: bigram mle for alice,in: 0.021341463414634148\n"
     ]
    }
   ],
   "source": [
    "#maximum likelihood estimation\n",
    "\n",
    "#unigrams\n",
    "unig_mle={}\n",
    "for i in unigrams.keys():\n",
    "    unig_mle[i]=unigrams[i]/tokens\n",
    "#example\n",
    "print(\"Example: unigram mle for alice:\",unig_mle[\"alice\"])\n",
    "#bigrams\n",
    "big_mle={}\n",
    "for (i,j) in bigrams.keys():\n",
    "    big_mle[(i,j)]=bigrams[(i,j)]/unigrams[i]\n",
    "#print(big_mle) \n",
    "#example\n",
    "print(\"Example: bigram mle for alice,in:\",big_mle[(\"alice\",\"in\")])\n",
    "\n",
    "#trigrams\n",
    "trig_mle={}\n",
    "for (i,j,k) in trigrams.keys():\n",
    "    trig_mle[(i,j,k)]=trigrams[(i,j,k)]/bigrams[(i,j)]\n",
    "\n",
    "\n",
    "#quadgrams\n",
    "quad_mle={}\n",
    "for (i,j,k,l) in quadgrams.keys():\n",
    "    quad_mle[(i,j,k,l)]=quadgrams[(i,j,k,l)]/trigrams[(i,j,k)]\n",
    "#print(quad_mle)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# number of possible ngrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of possible unigrams =  2446\n",
      "actual number of unigrams = 2446\n",
      "number of possible bigrams =  5982916\n",
      "actual number of bigrams = 12068\n",
      "number of possible trigrams =  14634212536\n",
      "actual number of trigrams = 18893\n",
      "number of possible quadgrams =  35795283863056\n",
      "actual number of quadgrams = 21599\n"
     ]
    }
   ],
   "source": [
    "print(\"number of possible unigrams = \",V)\n",
    "print(\"actual number of unigrams =\",V)\n",
    "\n",
    "print(\"number of possible bigrams = \",V**2)\n",
    "print(\"actual number of bigrams =\",len(bigrams))\n",
    "\n",
    "print(\"number of possible trigrams = \",V**3)\n",
    "print(\"actual number of trigrams =\",len(trigrams))\n",
    "\n",
    "print(\"number of possible quadgrams = \",V**4)\n",
    "print(\"actual number of quadgrams =\",len(quadgrams))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# sentence generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "def findnextword(prob,next_word): #function to predict next word using multinomial distribution\n",
    "    s=0\n",
    "    for i in prob:\n",
    "        s+=i\n",
    "    for i in prob:\n",
    "        i=i/s\n",
    "    #print(next_word)\n",
    "    a=np.random.multinomial(1,prob,size=1).tolist()\n",
    "    #print(a)\n",
    "    return next_word[a[0].index(1)]\n",
    "\n",
    "def bigprediction(cw): #predicted word using bigrams\n",
    "    prob=[]\n",
    "    next_word=[]\n",
    "    for (b1,b2) in big_mle.keys():\n",
    "        if (b1==cw):\n",
    "            prob.append(big_mle[(b1,b2)])\n",
    "            next_word.append(b2)\n",
    "    return prob,next_word\n",
    "\n",
    "def trigprediction(cw,cw1): #predicted word using trigrams\n",
    "    prob=[]\n",
    "    next_word=[]\n",
    "    for (t1,t2,t3) in trig_mle.keys():\n",
    "        if ((t1,t2)==(cw,cw1)):\n",
    "            prob.append(trig_mle[(t1,t2,t3)])\n",
    "            next_word.append(t3)\n",
    "    return prob,next_word\n",
    "\n",
    "def quadprediction(cw,cw1,cw2): ##predicted word using quadgrams\n",
    "    prob=[]\n",
    "    next_word=[]\n",
    "    for (q1,q2,q3,q4) in quad_mle.keys():\n",
    "        if ((q1,q2,q3)==(cw,cw1,cw2)):\n",
    "            prob.append(quad_mle[(q1,q2,q3,q4)])\n",
    "            next_word.append(q4)\n",
    "    return prob,next_word"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bigram generated sentences:\n",
      "<s> they were obliged to put them word sounded best butter wouldn\n",
      "<s> the poor child the antipathies i can draw the pigeon raising its mouth but it away consider your\n",
      "<s> behead that cheshire cat she was going back by the\n",
      "<s> don t remember what she had been the\n",
      "<s> anything that it to say that walk with\n",
      "\n",
      " Trigram generated sentences:\n",
      "<s> she pitied him deeply </s>\n",
      "<s> but perhaps it was even before she gave one sharp kick and waited till she had accidentally upset the\n",
      "<s> where are you content now said the dormouse </s>\n",
      "<s> you know this sort in her brother s latin\n",
      "<s> however she waited for some minutes </s>\n",
      "\n",
      " Quadgram generated sentences:\n",
      "<s> alice thought she had never before seen a rabbit with either a waistcoat-pocket or a watch to take out of\n",
      "<s> visit either you like they re both mad. but i don\n",
      "<s> never heard of uglification ” alice ventured to remark </s>\n",
      "<s> she stretched herself up on tiptoe and peeped over the edge\n",
      "<s> i quite forgot you didn t like cats. not like cats cried the mouse in a shrill loud voice and\n"
     ]
    }
   ],
   "source": [
    "def sent_gen(n): #n=2 for bigram, 3 for trigram..\n",
    "    sent_length=random.randint(8,20) #length of sentence\n",
    "    sentence=[\"<s>\"]\n",
    "    length=0\n",
    "    cw,cw1,cw2=\"<s>\",\"\",\"\"  #current words 0,1,2 for predicting next word\n",
    "    \n",
    "    if(n==2):\n",
    "        while(length<sent_length):\n",
    "            prob,next_word=bigprediction(cw)\n",
    "            f=findnextword(prob,next_word)\n",
    "            \n",
    "            sentence.append(f)\n",
    "            cw=f\n",
    "            length+=1\n",
    "            if(cw==\"</s>\"):\n",
    "                break\n",
    "    \n",
    "    if(n==3):\n",
    "        prob,next_word=bigprediction(cw)  #predict first word using bigram\n",
    "        f=findnextword(prob,next_word)\n",
    "        sentence.append(f)\n",
    "        cw1=f\n",
    "        length+=1\n",
    "        \n",
    "        while(length<sent_length): #predict all other words using trigram\n",
    "            prob,next_word=trigprediction(cw,cw1)\n",
    "            f=findnextword(prob,next_word)\n",
    "            sentence.append(f)\n",
    "            cw=cw1\n",
    "            cw1=f\n",
    "            length+=1\n",
    "            if(cw1==\"</s>\"):\n",
    "                break\n",
    "    if(n==4):\n",
    "        prob,next_word=bigprediction(cw) #predict first word using bigram\n",
    "        f=findnextword(prob,next_word)\n",
    "        sentence.append(f)\n",
    "        cw1=f\n",
    "        \n",
    "        prob,next_word=trigprediction(cw,cw1) #predict second word using trigram\n",
    "        f=findnextword(prob,next_word)\n",
    "        sentence.append(f)\n",
    "        cw2=f\n",
    "        length+=2\n",
    "        \n",
    "        while(length<sent_length): #predict all other words using quadgram\n",
    "            prob,next_word=quadprediction(cw,cw1,cw2)\n",
    "            f=findnextword(prob,next_word)\n",
    "            sentence.append(f)\n",
    "            cw=cw1\n",
    "            cw1=cw2\n",
    "            cw2=f\n",
    "            length+=1\n",
    "            if(cw2==\"</s>\"):\n",
    "                break\n",
    "    s=\" \".join(sentence)\n",
    "    print(s)\n",
    "print(\"Bigram generated sentences:\")        \n",
    "for i in range(5):\n",
    "    sent_gen(2) \n",
    "print(\"\\n Trigram generated sentences:\")  \n",
    "for i in range(5):\n",
    "    sent_gen(3) \n",
    "print(\"\\n Quadgram generated sentences:\")  \n",
    "for i in range(5):\n",
    "    sent_gen(4)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# log space probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "examples:\n",
      "‘what is his sorrow?’ she asked the gryphon, and the gryphon answered, very nearly in the same words as before, ‘it’s all his fancy, that: he hasn’t got no sorrow, you know.\n",
      "1.5439304633448217e-10\n",
      "alice opened the door and found that it led into a small passage, not much larger than a rat-hole: she knelt down and looked along the passage into the loveliest garden you ever saw.\n",
      "6.334531997918034e-21\n"
     ]
    }
   ],
   "source": [
    "def probability(n,sentence):\n",
    "    words=sentence.split()\n",
    "    prob=0\n",
    "    for i in range(len(words)-n):\n",
    "        try:    #for ngrams that do not exist in ngram list, assume log(p) to be zero.\n",
    "            if(n==1):\n",
    "                prob+=np.log(unig_mle[words[i]])\n",
    "            if(n==2):\n",
    "                prob+=np.log(big_mle[(words[i],words[i+1])])\n",
    "            if(n==3):\n",
    "                prob+=np.log(trig_mle[(words[i],words[i+1],words[i+2])])\n",
    "            if(n==4):\n",
    "                prob+=np.log(trig_mle[(words[i],words[i+1],words[i+2],words[i+3])])\n",
    "        except:\n",
    "            continue\n",
    "    print(10**prob)\n",
    "#example\n",
    "print(\"examples:\")\n",
    "sentence=test_dataset[5]\n",
    "print(sentence)\n",
    "probability(3,sentence)\n",
    "sentence=test_dataset[9]\n",
    "print(sentence)\n",
    "probability(3,sentence)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# add 1 smoothing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Bigram               Difference in count for mle and add one\n",
      "('</s>', '<s>') :         590.4073155610663\n",
      "('said', 'the') :         140.06514773940904\n",
      "('of', 'the') :         85.21783574198096\n",
      "('said', 'alice') :         82.5941616233535\n",
      "('<s>', 'i') :         68.75573465592065\n",
      "('in', 'a') :         60.91269258987528\n",
      "('it', 'was') :         52.30266343825666\n",
      "('in', 'the') :         51.04255319148936\n",
      "('and', 'the') :         49.94200576738225\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for mle's, add 1 to count of bigram and V to total count.\n",
    "\n",
    "big_mle1={}\n",
    "bigrams1={} #effective count after smoothing\n",
    "for (i,j) in bigrams.keys():\n",
    "    big_mle1[(i,j)]=(bigrams[(i,j)]+1)/(unigrams[i]+V)\n",
    "    bigrams1[(i,j)]=unigrams[i]*big_mle1[(i,j)]\n",
    "#print(big_mle1)\n",
    "l=[(bigrams[(i,j)]-bigrams1[(i,j)],i,j) for (i,j) in bigrams.keys()]\n",
    "l.sort(reverse=True)\n",
    "print(\"   Bigram\",\"              Difference in count for mle and add one\")\n",
    "for i in range(9):\n",
    "    print(l[i][1:],\":        \",l[i][0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The drastic change in counts post smoothing is due to unequal distribution of the added one count to various bigrams. For bigrams with large counts, adding just 1 and dividing by V affects the effective count the most, leading to maximum difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# good Turing "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22979\n",
      "c: 0 , d: -0.39875538535184296\n",
      "c: 1 , d: 0.6795809232784022\n",
      "c: 2 , d: 0.9414168937329701\n",
      "c: 3 , d: 0.9382239382239383\n",
      "c: 4 , d: 1.3408239700374533\n",
      "c: 5 , d: 0.436619718309859\n",
      "c: 6 , d: 1.1388888888888893\n",
      "c: 7 , d: -0.040000000000000036\n",
      "c: 8 , d: 2.954545454545454\n",
      "c: 9 , d: -0.18918918918918948\n",
      "Value of d for good turing smoothing is 0.7802155212475934\n"
     ]
    }
   ],
   "source": [
    "def good_turing(n):\n",
    "    #adjusted counts\n",
    "    #N[c]= number of bigrams seen exactly c times \n",
    "    N={}\n",
    "    N[0]=0\n",
    "    for (i,j) in bigrams:\n",
    "        c=bigrams[(i,j)]\n",
    "        N[c]=N.get(c,0)+1\n",
    "        N[0]+=c\n",
    "    print(N[0])\n",
    "    \n",
    "    #Turing_count[c]=effective turing count of bigrams seen exactly c times\n",
    "    Turing_count={}\n",
    "    Turing_count[11]=N[11] #for the last element in N.\n",
    "    c=0\n",
    "    avg=0\n",
    "    while (c<10):\n",
    "        Turing_count[c]= ((c+1)*N[c+1])/N[c]\n",
    "        avg+=(c-Turing_count[c])\n",
    "        print(\"c:\", c, \", d:\", c-Turing_count[c])\n",
    "        c+=1\n",
    "    d=avg/10\n",
    "    print(\"Value of d for good turing smoothing is\", d) \n",
    "good_turing(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "perplexity for add 1 smoothing 522.5550762319765\n",
      "perplexity for good turing smoothing 12194.885974612633\n"
     ]
    }
   ],
   "source": [
    "#test_dataset=sent_tokenize_list[k_train::]\n",
    "test_words=tokenize(test_dataset)\n",
    "#print(len(test_dataset))\n",
    "#perplexity for add 1 smoothing\n",
    "def perplexity(test_words,big_mle1):\n",
    "    p=1\n",
    "    for i in range(len(test_words)-2):\n",
    "        try:\n",
    "            p*=(1/(big_mle1[(test_words[i],test_words[i+1])]))**(1/len(test_words))\n",
    "            #print(p,\"1\\n\")\n",
    "        except:\n",
    "            prob=1/(unigrams[i]+V)\n",
    "            p*=(1/prob)**(1/len(test_words))\n",
    "            #print(p,\"2\\n\")\n",
    "    \n",
    "    return p\n",
    "print(\"perplexity for add 1 smoothing\",perplexity(test_words,big_mle1))\n",
    "\n",
    "N=[]\n",
    "N.append(22979)\n",
    "#perplexity for good turing\n",
    "def perplexityturing(test_words,d,bigrams):\n",
    "    p=1\n",
    "    #print(d)\n",
    "    for i in range(len(test_words)-2):\n",
    "        try:\n",
    "            b=(test_words[i],test_words[i+1])\n",
    "            c=bigrams[b]\n",
    "            if(c>d):\n",
    "                cstar=c-d\n",
    "            else:\n",
    "                cstar=d-c\n",
    "            prob=cstar/N[0]\n",
    "            #print(cstar)\n",
    "        except:\n",
    "            prob=1.0/N[0]\n",
    "            #print(\"hi\")\n",
    "        p*=(1/prob)**(1/len(test_words))\n",
    "    \n",
    "    return p\n",
    "print(\"perplexity for good turing smoothing\",perplexityturing(test_words,0.780,bigrams))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
